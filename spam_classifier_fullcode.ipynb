{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "def load_dataset(name_url):\n",
    "    root_url = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    url = root_url + name_url\n",
    "    tarball_path = Path(f\"email/{name_url.split('.')[0]}\")  # Directory to extract files\n",
    "    # Create root email directory if it doesn't exist\n",
    "    Path(\"email\").mkdir(parents=True, exist_ok=True)\n",
    "    # Download the tar file if it doesn't exist\n",
    "    if not tarball_path.is_file():\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as data_tarball:\n",
    "            data_tarball.extractall(path=\"email\")\n",
    "    # Return the path to the extracted directory containing emails\n",
    "\n",
    "# Datasets\n",
    "easy_ham = [\"20030228_easy_ham.tar.bz2\", \"20030228_easy_ham_2.tar.bz2\"]\n",
    "hard_ham = [\"20030228_hard_ham.tar.bz2\"]\n",
    "spam = [\"20030228_spam.tar.bz2\", \"20050311_spam_2.tar.bz2\"]\n",
    "\n",
    "for name_url in easy_ham+hard_ham+spam:\n",
    "    load_dataset(name_url)\n",
    "\n",
    "easy_ham_path = [f for f in sorted(Path(\"email/easy_ham\").iterdir()) if len(f.name) > 20] + [f for f in sorted(Path(\"email/easy_ham_2\").iterdir()) if len(f.name) > 20]\n",
    "hard_ham_path = [f for f in sorted(Path(\"email/hard_ham\").iterdir()) if len(f.name) > 20] \n",
    "spam_path = [f for f in sorted(Path(\"email/spam\").iterdir()) if len(f.name) > 20] + [f for f in sorted(Path(\"email/spam_2\").iterdir()) if len(f.name) > 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "from email import policy\n",
    "\n",
    "def load_email(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "easy_ham_emails = [load_email(f) for f in easy_ham_path]\n",
    "hard_ham_emails = [load_email(f) for f in hard_ham_path]\n",
    "spam_emails = [load_email(f) for f in spam_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n",
      "250\n",
      "1896\n"
     ]
    }
   ],
   "source": [
    "print(len(easy_ham_emails))\n",
    "print(len(hard_ham_emails))\n",
    "print(len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Martin A posted:\\nTassos Papadopoulos, the Greek sculptor behind the plan, judged that the\\n limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\\n Mount Athos monastic community, was ideal for the patriotic sculpture. \\n \\n As well as Alexander's granite features, 240 ft high and 170 ft wide, a\\n museum, a restored amphitheatre and car park for admiring crowds are\\nplanned\\n---------------------\\nSo is this mountain limestone or granite?\\nIf it's limestone, it'll weather pretty fast.\\n\\n------------------------ Yahoo! Groups Sponsor ---------------------~-->\\n4 DVDs Free +s&p Join Now\\nhttp://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\\n---------------------------------------------------------------------~->\\n\\nTo unsubscribe from this group, send an email to:\\nforteana-unsubscribe@egroups.com\\n\\n \\n\\nYour use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_ham_emails[1].get_content().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        multipart = \", \".join([get_email_structure(sub_email) for sub_email in payload])\n",
    "        return f\"multipart({multipart})\"\n",
    "    return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/plain': 96.17948717948681,\n",
       "         'multipart(text/plain, application/pgp-signature)': 2.5897435897435845,\n",
       "         'multipart(text/plain, text/html)': 0.512820512820513,\n",
       "         'multipart(text/plain, text/plain)': 0.10256410256410256,\n",
       "         'multipart(text/plain)': 0.07692307692307693,\n",
       "         'multipart(text/plain, application/ms-tnef, text/plain)': 0.05128205128205128,\n",
       "         'multipart(text/plain, application/octet-stream)': 0.05128205128205128,\n",
       "         'multipart(text/plain, multipart(text/plain))': 0.05128205128205128,\n",
       "         'multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)': 0.05128205128205128,\n",
       "         'text/html': 0.05128205128205128,\n",
       "         'multipart(text/plain, text/enriched)': 0.02564102564102564,\n",
       "         'multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)': 0.02564102564102564,\n",
       "         'multipart(text/plain, video/mng)': 0.02564102564102564,\n",
       "         'multipart(text/plain, application/x-pkcs7-signature)': 0.02564102564102564,\n",
       "         'multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))': 0.02564102564102564,\n",
       "         'multipart(text/plain, application/x-java-applet)': 0.02564102564102564,\n",
       "         'multipart(text/plain, application/x-patch)': 0.02564102564102564,\n",
       "         'multipart(multipart(text/plain, multipart(text/plain), text/plain), application/pgp-signature)': 0.02564102564102564,\n",
       "         'multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/gif, image/gif)': 0.02564102564102564,\n",
       "         'multipart(text/plain, application/ms-tnef)': 0.02564102564102564,\n",
       "         'multipart(text/plain, text/plain, text/plain)': 0.02564102564102564})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails, len):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 100/len\n",
    "    return structures\n",
    "\n",
    "structures_counter(easy_ham_emails, 3900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/html': 47.199999999999896,\n",
       "         'text/plain': 32.39999999999995,\n",
       "         'multipart(text/plain, text/html)': 17.200000000000003,\n",
       "         'multipart(text/html)': 0.8,\n",
       "         'multipart(text/plain, image/bmp)': 0.4,\n",
       "         'multipart(multipart(text/plain, text/html))': 0.4,\n",
       "         'multipart(text/plain, application/x-pkcs7-signature)': 0.4,\n",
       "         'multipart(text/plain, image/png, image/png)': 0.4,\n",
       "         'multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif)': 0.4,\n",
       "         'multipart(text/plain, text/plain)': 0.4})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(hard_ham_emails, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/plain': 42.985232067510005,\n",
       "         'text/html': 40.71729957805858,\n",
       "         'multipart(text/plain, text/html)': 8.386075949367074,\n",
       "         'multipart(text/html)': 2.5843881856540087,\n",
       "         'multipart(text/plain)': 2.3206751054852317,\n",
       "         'multipart(multipart(text/html))': 1.2130801687763713,\n",
       "         'multipart(multipart(text/plain, text/html))': 0.26371308016877637,\n",
       "         'multipart(text/plain, application/octet-stream)': 0.15822784810126583,\n",
       "         'multipart(text/html, text/plain)': 0.15822784810126583,\n",
       "         'multipart(text/plain, image/jpeg)': 0.15822784810126583,\n",
       "         'multipart(text/plain, application/octet-stream, text/plain)': 0.15822784810126583,\n",
       "         'multipart(text/html, application/octet-stream)': 0.10548523206751055,\n",
       "         'multipart/alternative': 0.10548523206751055,\n",
       "         'multipart(text/html, image/jpeg)': 0.10548523206751055,\n",
       "         'multipart(multipart(text/plain), application/octet-stream)': 0.10548523206751055,\n",
       "         'multipart(multipart(text/html), application/octet-stream, image/jpeg)': 0.052742616033755275,\n",
       "         'multipart(multipart(text/plain, text/html), image/gif)': 0.052742616033755275,\n",
       "         'multipart(text/plain, multipart(text/plain))': 0.052742616033755275,\n",
       "         'multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)': 0.052742616033755275,\n",
       "         'multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/gif)': 0.052742616033755275,\n",
       "         'text/plain charset=us-ascii': 0.052742616033755275,\n",
       "         'multipart(multipart(text/html), image/gif)': 0.052742616033755275,\n",
       "         'multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)': 0.052742616033755275,\n",
       "         'multipart(multipart(text/plain, text/html), image/gif, image/jpeg)': 0.052742616033755275})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails, 1896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spam have more html ones 53% while easy ham have just 0.51% but hard ham has 66% html. so of a email is a multipart or has a html content then it more probability of being a SPAM but if a email is php sihnatured it is more likely a ham email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <FreeSoftware-5265v80@yahoo.com>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id BE46143F9B\tfor <zzzz@localhost>; Mon, 26 Aug 2002 16:37:20 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Mon, 26 Aug 2002 21:37:20 +0100 (IST)\n",
      "Received : from yahoo.com ([211.185.47.189])\tby webnote.net (8.9.3/8.9.3) with SMTP id VAA27898\tfor <zzzz@spamassassin.taint.org>; Mon, 26 Aug 2002 21:39:06 +0100\n",
      "Reply-To : Free Publishing Software <FreeSoftware-5265v80@yahoo.com>\n",
      "Message-ID : <004b12e28d1a$4347d2b7$3ce68ab0@sgcrua>\n",
      "From : Free Publishing Software <FreeSoftware-5265v80@yahoo.com>\n",
      "To : zzzz@spamassassin.taint.org\n",
      "Subject : Take your Marketing to the Next Level\n",
      "Date : Mon, 26 Aug 2002 19:24:06 +0100\n",
      "MiME-Version : 1.0\n",
      "X-Priority : 3 (Normal)\n",
      "X-MSMail-Priority : Normal\n",
      "X-Mailer : Microsoft Outlook Express 5.00.2919.6700\n",
      "Importance : Normal\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[100].items():\n",
    "    print(header, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Publishing Software <FreeSoftware-5265v80@yahoo.com>\n",
      "Take your Marketing to the Next Level\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[100][\"From\"])\n",
    "print(spam_emails[100][\"Subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(easy_ham_emails + spam_emails, dtype=\"object\")\n",
    "Y = np.array([0]*len(easy_ham_emails) + [1]*len(spam_emails))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_to_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    total_content = \"\"\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                content = part.get_content()\n",
    "            except:\n",
    "                content = str(part.get_payload())\n",
    "            if ctype == \"text/plain\":\n",
    "                total_content += content\n",
    "            else:\n",
    "                total_content += html_to_text(content)\n",
    "    return total_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details\n",
      "\n",
      "\n",
      "\n",
      "Want to refinance?\n",
      "\n",
      "Fill our this quick form and immediately have mortgage\n",
      "companies compete for you business. \n",
      "\n",
      "You will be offered the, absolute, BEST refinance rates \n",
      "availible!\n",
      "\n",
      "Your credit doesn't matter, don't even worry about past \n",
      "credit problems, we can refinance ANYONE!\n",
      "\n",
      "Let Us Put Our Expertise to Work for You!\n",
      "\n",
      "http://210.51.251.244/al/cgi-bin/redir.cgi?goto=ID74210\n",
      "\n",
      "Or Site 2\n",
      "http://61.129.81.99/al/cgi-bin/redir.cgi?goto=ID74215\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Erase\n",
      "http://210.51.251.244/al/uns/list.htm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(X_train[195]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fell\n",
      "fall\n",
      "fallen\n",
      "felt\n",
      "feel\n",
      "taken\n",
      "took\n",
      "take\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "for word in [\"fell\", \"fall\", \"fallen\", \"felt\", \"feel\", \"taken\", \"took\", \"taking\"]:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\huzai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run', 'happiness', 'be']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"ran\", \"runs\", \"run\", \"happiness\", \"was\"]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]  # pos=\"v\" for verbs\n",
    "\n",
    "print(lemmas)  # Output: ['run', 'run', 'run', 'run', 'happiness', 'be']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://210.51.251.244/al/cgi-bin/redir.cgi?goto=ID74210', 'http://61.129.81.99/al/cgi-bin/redir.cgi?goto=ID74215', 'http://210.51.251.244/al/uns/list.htm']\n"
     ]
    }
   ],
   "source": [
    "from urlextract import URLExtract\n",
    "def url_extractor(email):\n",
    "    url_extract = URLExtract()\n",
    "    urls = url_extract.find_urls(email)\n",
    "    return urls\n",
    "\n",
    "print(url_extractor(email_to_text(X_train[195])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import re\n",
    "\n",
    "class EmailtoWordCounterTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, strip_headers=True, to_lowercase=True, replace_numbers=True,\n",
    "                remove_punctuations=True, replace_urls=True, stemming=True):\n",
    "        self.strip_headers=strip_headers\n",
    "        self.to_lowercase=to_lowercase\n",
    "        self.replace_numbers=replace_numbers\n",
    "        self.replace_urls = replace_urls\n",
    "        self.remove_punctuations = remove_punctuations\n",
    "        self.stemming=stemming\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text=email_to_text(email) or \"\"\n",
    "            if self.to_lowercase:\n",
    "                text=text.lower()\n",
    "            if self.replace_urls:\n",
    "                if url_extractor is None:\n",
    "                    raise ValueError(\"URL extractor is not initialized!\")\n",
    "                urls = url_extractor(text)\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text=re.sub(\"\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+(?:\\.\\d*))?\", \"NUMBER\", text)\n",
    "            if self.remove_punctuations:\n",
    "                text=re.sub(\"\\W+\", ' ', text)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming:\n",
    "                if porter is None:\n",
    "                    raise ValueError(\"Porter Stemmer is not initialized!\")\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word_counts[porter.stem(word)] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([Counter({'i': 9, 'of': 9, 'the': 9, 'your': 8, 'for': 7, 'thi': 6, 'in': 5, 'a': 5, 'to': 5, 'you': 5, 'money': 5, 'my': 4, 'with': 4, 'on': 4, 'contact': 3, 'assist': 3, 'invest': 3, 'mr': 3, 'jallow': 3, 'and': 3, 'mail': 3, 'url': 3, 'search': 2, 'which': 2, 'transfer': 2, 'm': 2, 'ibrahim': 2, 'late': 2, 'unit': 2, 'u': 2, 'wa': 2, 'number': 2, 's': 2, 'realis': 2, 'can': 2, 'through': 2, 'e': 2, 'hello': 1, 'dear': 1, 'sir': 1, 'got': 1, 'caus': 1, 'serious': 1, 'reliabl': 1, 'foreign': 1, 'partner': 1, 'realli': 1, 'made': 1, 'me': 1, 'purpos': 1, 'son': 1, 'sierraleonian': 1, 'busi': 1, 'man': 1, 'kulu': 1, 'who': 1, 'die': 1, 'two': 1, 'yaer': 1, 'ago': 1, 'when': 1, 'revolutionari': 1, 'front': 1, 'rebel': 1, 'r': 1, 'f': 1, 'attack': 1, 'our': 1, 'resid': 1, 'makeni': 1, 'sierra': 1, 'leon': 1, 'follow': 1, 'ceas': 1, 'fire': 1, 'agreement': 1, 'reach': 1, 'last': 1, 'year': 1, 'help': 1, 'nation': 1, 'peac': 1, 'keep': 1, 'troop': 1, 'use': 1, 'oppoturn': 1, 'leav': 1, 'countri': 1, 'veri': 1, 'import': 1, 'document': 1, 'us': 1, 'numberm': 1, 'fourteen': 1, 'million': 1, 'five': 1, 'hundr': 1, 'thounsand': 1, 'dollar': 1, 'deposit': 1, 'by': 1, 'father': 1, 'secur': 1, 'compani': 1, 'dakar': 1, 'seneg': 1, 'under': 1, 'name': 1, 'from': 1, 'diamond': 1, 'export': 1, 'now': 1, 'trust': 1, 'individu': 1, 'or': 1, 'fore': 1, 'firm': 1, 'whom': 1, 'am': 1, 'next': 1, 'kin': 1, 'these': 1, 'howev': 1, 'base': 1, 'capabl': 1, 'vast': 1, 'knowledg': 1, 'intern': 1, 'commerci': 1, 'co': 1, 'opert': 1, 'will': 1, 'give': 1, 'total': 1, 'sum': 1, 'after': 1, 'sucessful': 1, 'pleas': 1, 'kindli': 1, 'commun': 1, 'accept': 1, 'propos': 1, 'address': 1, 'so': 1, 'that': 1, 'we': 1, 'discuss': 1, 'modal': 1, 'see': 1, 'transact': 1, 'count': 1, 'great': 1, 'await': 1, 'earnest': 1, 'respons': 1, 'regard': 1, 'faith': 1, '_____________________________________________________________________': 1, 'webdunia': 1, 'quiz': 1, 'contest': 1, 'limit': 1, 'time': 1, 'unlimit': 1, 'fun': 1, 'log': 1, 'win': 1, 'fabul': 1, 'prize': 1, 'india': 1, 'first': 1, 'multilingu': 1, 'system': 1, 'get': 1, 'free': 1, 'account': 1, 'at': 1}),\n",
      "       Counter({'number': 46, 'hextab': 21, 'i': 14, 'the': 10, 'c': 10, 'cnumber': 8, 'in': 5, 'str': 5, 'if': 5, 's': 4, 'len': 4, 'decod': 4, 'on': 3, 'not': 3, 'wa': 3, 'to': 3, 'for': 3, 'and': 3, 'code': 3, 'hex': 3, 'substr': 3, 'my': 3, 'paul': 3, 'ie': 3, 'linux': 3, 'echo': 2, 'e': 2, 'f': 2, 'x': 2, 'whi': 2, 'someth': 2, 'it': 2, 'a': 2, 'end': 2, 'check': 2, 'encod': 2, 'toupper': 2, 'is': 2, 'footer': 2, 'mail': 2, 'yadda': 2, 'spam': 2, 'he': 2, 'jakma': 2, 'tue': 1, 'aug': 1, 'david': 1, 'neari': 1, 'wrote': 1, 'actual': 1, 'follow': 1, 'would': 1, 'be': 1, 'some': 1, 'way': 1, 'sensibl': 1, 'enc': 1, 'sed': 1, 'numbera': 1, 'fa': 1, 'g': 1, 'no': 1, 'idea': 1, 'abov': 1, 'along': 1, 'line': 1, 'attempt': 1, 'onc': 1, 'realis': 1, 'straight': 1, 'swap': 1, 'but': 1, 'couldnt': 1, 'get': 1, 'awk': 1, 'gensub': 1, 'insert': 1, 'anyway': 1, 'found': 1, 'internet': 1, 'adapt': 1, 'function': 1, 'decode_url': 1, 'dec': 1, 'lookup': 1, 'tabl': 1, 'b': 1, 'd': 1, 'length': 1, 'while': 1, 'usual': 1, 'start': 1, 'of': 1, 'uri': 1, 'char': 1, 'valid': 1, 'sprintf': 1, 'space': 1, 'appar': 1, 'els': 1, 'return': 1, 'cheer': 1, 'dave': 1, 'ps': 1, 'late': 1, 'repli': 1, 'becaus': 1, 'origin': 1, 'you': 1, 'receiv': 1, 'thi': 1, 'error': 1, 'got': 1, 'caught': 1, 'filter': 1, 'up': 1, 'junkmail': 1, 'directori': 1, 'might': 1, 'have': 1, 'been': 1, 'header': 1, 'regard': 1, 'clubi': 1, 'org': 1, 'key': 1, 'id': 1, 'numberanumberffnumbera': 1, 'warn': 1, 'do': 1, 'ever': 1, 'send': 1, 'email': 1, 'dishon': 1, 'st': 1, 'fortun': 1, 'one': 1, 'nuclear': 1, 'bomb': 1, 'can': 1, 'ruin': 1, 'your': 1, 'whole': 1, 'day': 1, 'irish': 1, 'user': 1, 'group': 1, 'ilug': 1, 'url': 1, 'un': 1, 'subscript': 1, 'inform': 1, 'list': 1, 'maintain': 1, 'listmast': 1}),\n",
      "       Counter({'to': 6, 'the': 6, 'it': 6, 'sell': 4, 'doe': 4, 'i': 4, 'book': 4, 'what': 3, 'pratchett': 3, 'read': 3, 'and': 3, 'give': 3, 'someon': 3, 'stori': 2, 'when': 2, 'have': 2, 'of': 2, 'mr': 2, 'who': 2, 'then': 2, 'are': 1, 'you': 1, 'tri': 1, 'is': 1, 'valu': 1, 'exampl': 1, 'paper': 1, 'bound': 1, 'by': 1, 'glue': 1, 'or': 1, 'he': 1, 'question': 1, 'buy': 1, 'a': 1, 'purchas': 1, 'ani': 1, 'that': 1, 'revenu': 1, 'go': 1, 'if': 1, 'url': 1, 'though': 1, 'with': 1, 'more': 1, 'succesful': 1, 'pass': 1, 'each': 1, 'reader': 1, 'send': 1, 'money': 1, 'use': 1, 'bookstor': 1, 'recorstor': 1, 'etc': 1, 'destroy': 1, 'system': 1, 'record': 1, 'economi': 1, 'as': 1, 'resid': 1, 'sourpuss': 1, 'in': 1, 'germani': 1, 'bitter': 1, 'may': 1, 'be': 1, 'better': 1, 'but': 1, 'here': 1, 'just': 1, 'plain': 1, 'stinkin': 1, 'thinkin': 1, 'tom': 1}),\n",
      "       Counter({'number': 14, 'mit': 3, 'and': 3, 'the': 3, 'they': 3, 'i': 2, 'from': 2, 'origin': 2, 'coursewar': 2, 'effort': 2, 'at': 2, 'up': 2, 'use': 2, 'b': 2, 'k': 2, 'delong': 2, 'bkdelong': 2, 'to': 2, 'eugen': 2, 'leitl': 2, 'opencoursewar': 2, 'll': 2, 'better': 1, 'late': 1, 'than': 1, 'never': 1, 'receiv': 1, 'a': 1, 'grant': 1, 'project': 1, 'athena': 1, 's': 1, 'found': 1, 'end': 1, 'that': 1, 'hadn': 1, 't': 1, 'thought': 1, 'much': 1, 'about': 1, 'distribut': 1, 'of': 1, 'instead': 1, 'had': 1, 'gotten': 1, 'wrap': 1, 'with': 1, 'x': 1, 'variou': 1, 'unix': 1, 'tool': 1, 'other': 1, 'but': 1, 'not': 1, 'strictli': 1, 'educ': 1, 'ken': 1, 'messag': 1, 'mailto': 1, 'pobox': 1, 'com': 1, 'sent': 1, 'tuesday': 1, 'octob': 1, 'am': 1, 'forkit': 1, 'subject': 1, 're': 1, 'pm': 1, 'wrote': 1, 'look': 1, 'hope': 1, 'put': 1, 'some': 1, 'more': 1, 'materi': 1, 'soon': 1, 'url': 1, 'be': 1, 'sure': 1, 'keep': 1, 'everyon': 1, 'post': 1, 'on': 1, 'next': 1, 'updat': 1, 'ceci': 1, 'edu': 1, 'cell': 1}),\n",
      "       Counter({'to': 7, 'of': 5, 'our': 5, 'free': 4, 'for': 4, 'bonu': 3, 'qualiti': 3, 'replica': 3, 'from': 3, 'catalog': 3, 'you': 3, 'regist': 3, 'now': 3, 'out': 3, 'linux': 3, 'offer': 2, 'we': 2, 'good': 2, 'all': 2, 'design': 2, 'a': 2, 'the': 2, 'price': 2, 'receiv': 2, 'inform': 2, 'email': 2, 'fastnetspain': 2, 'net': 2, 'be': 2, 'sure': 2, 'don': 2, 't': 2, 'miss': 2, 'ie': 2, 'see': 1, 'below': 1, 'can': 1, 'suppli': 1, 'top': 1, 'virtual': 1, 'ident': 1, 'just': 1, 'about': 1, 'anyth': 1, 'watch': 1, 'wallet': 1, 'lighter': 1, 'lingeri': 1, 'cloth': 1, 'accessori': 1, 'even': 1, 'electr': 1, 'your': 1, 'favorit': 1, 'label': 1, 'reproduc': 1, 'at': 1, 'fraction': 1, 'major': 1, 'credit': 1, 'card': 1, 'accept': 1, 'worldwid': 1, 'certifi': 1, 'ship': 1, 'guarante': 1, 'are': 1, 'current': 1, 'build': 1, 'so': 1, 'let': 1, 'us': 1, 'know': 1, 'that': 1, 'want': 1, 'notif': 1, 'when': 1, 'on': 1, 'line': 1, 'is': 1, 'publish': 1, 'later': 1, 'thi': 1, 'month': 1, 'qualifi': 1, 'great': 1, 'and': 1, 'one': 1, 'pair': 1, 'sunglass': 1, 'regular': 1, 'number': 1, 'check': 1, 'charg': 1, 'more': 1, 'remov': 1, 'futur': 1, 'mail': 1, 'noreplica': 1, 'irish': 1, 'user': 1, 'group': 1, 'ilug': 1, 'url': 1, 'un': 1, 'subscript': 1, 'list': 1, 'maintain': 1, 'listmast': 1})],\n",
      "      dtype=object)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(EmailtoWordCounterTransformer().fit_transform(X_train[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCountertoVectorsTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        total_words = Counter()\n",
    "        for words_counter in X:\n",
    "            for word, count in words_counter.items():\n",
    "                total_words[word] += min(count, 10)\n",
    "        most_common = total_words.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word:index+1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        vector = []\n",
    "        col_index = []\n",
    "        row_index = []\n",
    "        for row, email_word_counter in enumerate(X):\n",
    "            for word, count in email_word_counter.items():\n",
    "                row_index.append(row)\n",
    "                col_index.append(self.vocabulary_.get(word, 0))\n",
    "                vector.append(count)\n",
    "        return csr_matrix((vector, (row_index, col_index)), shape=(len(X), self.vocabulary_size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x11 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 47 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TovectorTransformer = WordCountertoVectorsTransformer(vocabulary_size=10)\n",
    "sample_vector = TovectorTransformer.fit_transform(\n",
    "    EmailtoWordCounterTransformer().fit_transform(X_train[:5]))\n",
    "sample_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[217,   9,   9,   5,   2,   9,   7,   3,   5,   5,   8],\n",
       "       [236,  10,  14,   3,  46,   1,   3,   3,   5,   2,   1],\n",
       "       [ 98,   6,   4,   6,   0,   2,   0,   3,   1,   1,   0],\n",
       "       [101,   3,   2,   2,  14,   1,   0,   3,   0,   1,   0],\n",
       "       [145,   2,   0,   7,   1,   5,   4,   1,   0,   2,   1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'number': 4,\n",
       " 'of': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'a': 9,\n",
       " 'your': 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TovectorTransformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    (\"Words_to_Counter\", EmailtoWordCounterTransformer()),\n",
    "    (\"WordCounter_To_Vector\", WordCountertoVectorsTransformer())\n",
    "])\n",
    "X_train_processed = preprocessing.fit_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866281559544202"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_processed, Y_train, cv=10)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859778803902586"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "score = cross_val_score(rnd_clf, X_train_tfidf, Y_train, cv=10)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889974491695837"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(random_state=42)\n",
    "score = cross_val_score(svm_clf, X_train_tfidf, Y_train, cv=10)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9167409883071425"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "score = cross_val_score(knn_clf, X_train_tfidf, Y_train, cv=10)\n",
    "score.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857614321888732"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "score = cross_val_score(xgb_clf, X_train_tfidf, Y_train, cv=10)\n",
    "score.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 97.38%\n",
      "Recall 93.82%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[795,   9],\n",
       "       [ 22, 334]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test_processed = preprocessing.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_processed)\n",
    "log_clf.fit(X_train_processed, Y_train)\n",
    "Y_pred = log_clf.predict(X_test_processed)\n",
    "print(f\"Precision {precision_score(Y_test, Y_pred):.2%}\")\n",
    "print(f\"Recall {recall_score(Y_test, Y_pred):.2%}\")\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[150, 100],\n",
       "       [  0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_ham_predict = log_clf.predict(preprocessing.transform(np.array(hard_ham_emails, dtype=\"object\")))\n",
    "Y_orig = np.array([0]*len(hard_ham_emails))\n",
    "conf1 = confusion_matrix(Y_orig, hard_ham_predict)\n",
    "conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 98.87%\n",
      "Recall 98.60%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[800,   4],\n",
       "       [  5, 351]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test_processed = preprocessing.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_processed)\n",
    "xgb_clf.fit(X_train_tfidf, Y_train)\n",
    "Y_pred = xgb_clf.predict(X_test_tfidf)\n",
    "print(f\"Precision {precision_score(Y_test, Y_pred):.2%}\")\n",
    "print(f\"Recall {recall_score(Y_test, Y_pred):.2%}\")\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 86, 164],\n",
       "       [  0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_ham_predict = xgb_clf.predict(tfidf_transformer.fit_transform(preprocessing.transform(np.array(hard_ham_emails, dtype=\"object\"))))\n",
    "Y_orig = np.array([0]*len(hard_ham_emails))\n",
    "conf1 = confusion_matrix(Y_orig, hard_ham_predict)\n",
    "conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 99.43%\n",
      "Recall 98.03%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[802,   2],\n",
       "       [  7, 349]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test_processed = preprocessing.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_processed)\n",
    "rnd_clf.fit(X_train_tfidf, Y_train)\n",
    "Y_pred = rnd_clf.predict(X_test_tfidf)\n",
    "print(f\"Precision {precision_score(Y_test, Y_pred):.2%}\")\n",
    "print(f\"Recall {recall_score(Y_test, Y_pred):.2%}\")\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[134, 116],\n",
       "       [  0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_ham_predict = rnd_clf.predict(tfidf_transformer.fit_transform(preprocessing.transform(np.array(hard_ham_emails, dtype=\"object\"))))\n",
    "Y_orig = np.array([0]*len(hard_ham_emails))\n",
    "conf1 = confusion_matrix(Y_orig, hard_ham_predict)\n",
    "conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 98.60%\n",
      "Recall 98.88%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[799,   5],\n",
       "       [  4, 352]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test_processed = preprocessing.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_processed)\n",
    "svm_clf.fit(X_train_tfidf, Y_train)\n",
    "Y_pred = svm_clf.predict(X_test_tfidf)\n",
    "print(f\"Precision {precision_score(Y_test, Y_pred):.2%}\")\n",
    "print(f\"Recall {recall_score(Y_test, Y_pred):.2%}\")\n",
    "conf = confusion_matrix(Y_test, Y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[150, 100],\n",
       "       [  0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_ham_predict = svm_clf.predict(tfidf_transformer.fit_transform(preprocessing.transform(np.array(hard_ham_emails, dtype=\"object\"))))\n",
    "Y_orig = np.array([0]*len(hard_ham_emails))\n",
    "conf1 = confusion_matrix(Y_orig, hard_ham_predict)\n",
    "conf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this only three models were selected logistic regression, random forest, and SVC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
